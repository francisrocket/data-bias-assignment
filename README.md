# data-bias-assignment

In this project, I tested the Perspective API released by Google Jigsaw for biases, specifically, biases related to the length of the text. My hypothesis is that the Perspective API will make more mistakes in assessing the toxicity of a long piece of text compared to a short piece of text. I tested using a threshold of 0.45, or 45%, as the toxicity score needed to determine a comment to be toxic. 

From my tests, it's shown that the average length of a toxic comment is 179 characters, and the comment which was incorrectly labeled as toxic had a much greater length of 1208. This supports our hypothesis that the Perspective API would be more likely to make mistakes on assessing the toxicity of a long piece of text rather than make a mistake in assessing the toxicity of a short piece of text. Additionally, we also looked at the average length of a nontoxic comment, which was 192, and the comment which was incorrectly labeled as nontoxic had length of 203, similar in length but about 5% longer. This also supports our hypothesis that the Perspective API would be more likely to make mistakes on assessing the toxicity of a long piece of text rather than make a mistake in assessing the toxicity of a short piece of text.

A low sample size of 12 impacted our results by giving us few data points to work with. Mistakes in the first place seem to be relatively rare, as shown by finding only 1 mistake among the 12 data points and 2 among the 24 total data points. We can still begin to draw conclusions, but they should by no means be conclusive due to the low sample size. 

Conclusion (From Jupyter Notebook file):
Overall, I find this small project to be a very positive experience from which I learned a lot. I learned from the ground up how to use an API from Google Cloud and incorporate it in a project done from scratch. This involves getting the API, enabling it, figuring out how to use it (by getting an API key), and incorporating it into my code so that I can call it to perform it's various functions, in this case being a machine learning model which can score texts for toxicity.

A surprising result I got was that the API does in fact seem more accurate with short texts than long texts, and I had initially thought that there are reasons to support either side of that argument. Short texts could be more innacurate because there might be less context that the API can derive in order to make it's decision, and it doesn't have many total words to work with. On the other hand, long texts give the API plenty of material to work with, but as we saw in our tests, long text is still able to "trick" the API into making an inaccuracy by potentially including lots of filler and carefully incorporating the insults and toxicity in more subtle ways. This could suggest that the model may draw its conclusions a bit too quickly, and may benefit from training with data that contains different punctionation types that elongate the sentence.

To summarize, my hypothesis was that I thought the model would be more accurate with shorter text, and the bias that my tests suggested is in fact a bias towards shorter text, seeing that it was more accurate with shorter text than longer text. My theory for this bias is that there was less training data that included long text with punctations and rarer grammatical forms, and thus the model became less accurate on this type of data and more accurate with short text data.
